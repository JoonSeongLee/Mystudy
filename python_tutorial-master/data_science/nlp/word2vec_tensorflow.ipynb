{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "here I implement word2vec with very simple example using tensorflow  \n",
    "word2vec is vector representation for words with similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "we will use only 10 sentences to create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['king is a strong man', \n",
    "          'queen is a wise woman', \n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong', \n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "In order for efficiency of creating word vector, we will remove commonly used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(\" \".join(tmp))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = remove_stop_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king strong man',\n",
       " 'queen wise woman',\n",
       " 'boy young man',\n",
       " 'girl young woman',\n",
       " 'prince young king',\n",
       " 'princess young queen',\n",
       " 'man strong',\n",
       " 'woman pretty',\n",
       " 'prince boy king',\n",
       " 'princess girl queen']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have word set by which we will have word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy',\n",
       " 'girl',\n",
       " 'king',\n",
       " 'man',\n",
       " 'pretty',\n",
       " 'prince',\n",
       " 'princess',\n",
       " 'queen',\n",
       " 'strong',\n",
       " 'wise',\n",
       " 'woman',\n",
       " 'young'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generation\n",
    "we will generate label for each word using skip gram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy': 2,\n",
       " 'girl': 8,\n",
       " 'king': 5,\n",
       " 'man': 6,\n",
       " 'pretty': 3,\n",
       " 'prince': 9,\n",
       " 'princess': 1,\n",
       " 'queen': 10,\n",
       " 'strong': 11,\n",
       " 'wise': 7,\n",
       " 'woman': 4,\n",
       " 'young': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int = {}\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "\n",
    "\n",
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'strong', 'man'],\n",
       " ['queen', 'wise', 'woman'],\n",
       " ['boy', 'young', 'man'],\n",
       " ['girl', 'young', 'woman'],\n",
       " ['prince', 'young', 'king'],\n",
       " ['princess', 'young', 'queen'],\n",
       " ['man', 'strong'],\n",
       " ['woman', 'pretty'],\n",
       " ['prince', 'boy', 'king'],\n",
       " ['princess', 'girl', 'queen']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in corpus:\n",
    "    sentences.append(sentence.split())\n",
    "    \n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'strong'],\n",
       " ['king', 'man'],\n",
       " ['strong', 'king'],\n",
       " ['strong', 'man'],\n",
       " ['man', 'king'],\n",
       " ['man', 'strong'],\n",
       " ['queen', 'wise'],\n",
       " ['queen', 'woman'],\n",
       " ['wise', 'queen'],\n",
       " ['wise', 'woman'],\n",
       " ['woman', 'queen'],\n",
       " ['woman', 'wise'],\n",
       " ['boy', 'young'],\n",
       " ['boy', 'man'],\n",
       " ['young', 'boy'],\n",
       " ['young', 'man'],\n",
       " ['man', 'boy'],\n",
       " ['man', 'young'],\n",
       " ['girl', 'young'],\n",
       " ['girl', 'woman'],\n",
       " ['young', 'girl'],\n",
       " ['young', 'woman'],\n",
       " ['woman', 'girl'],\n",
       " ['woman', 'young'],\n",
       " ['prince', 'young'],\n",
       " ['prince', 'king'],\n",
       " ['young', 'prince'],\n",
       " ['young', 'king'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'young'],\n",
       " ['princess', 'young'],\n",
       " ['princess', 'queen'],\n",
       " ['young', 'princess'],\n",
       " ['young', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'young'],\n",
       " ['man', 'strong'],\n",
       " ['strong', 'man'],\n",
       " ['woman', 'pretty'],\n",
       " ['pretty', 'woman'],\n",
       " ['prince', 'boy'],\n",
       " ['prince', 'king'],\n",
       " ['boy', 'prince'],\n",
       " ['boy', 'king'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'boy'],\n",
       " ['princess', 'girl'],\n",
       " ['princess', 'queen'],\n",
       " ['girl', 'princess'],\n",
       " ['girl', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'girl']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 2\n",
    "\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "                \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king strong man\n",
      "queen wise woman\n",
      "boy young man\n",
      "girl young woman\n",
      "prince young king\n",
      "princess young queen\n",
      "man strong\n",
      "woman pretty\n",
      "prince boy king\n",
      "princess girl queen\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for text in corpus:\n",
    "    print(text)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strong</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>man</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queen</td>\n",
       "      <td>wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queen</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wise</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wise</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input   label\n",
       "0    king  strong\n",
       "1    king     man\n",
       "2  strong    king\n",
       "3  strong     man\n",
       "4     man    king\n",
       "5     man  strong\n",
       "6   queen    wise\n",
       "7   queen   woman\n",
       "8    wise   queen\n",
       "9    wise   woman"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy': 2,\n",
       " 'girl': 8,\n",
       " 'king': 5,\n",
       " 'man': 6,\n",
       " 'pretty': 3,\n",
       " 'prince': 9,\n",
       " 'princess': 1,\n",
       " 'queen': 10,\n",
       " 'strong': 11,\n",
       " 'wise': 7,\n",
       " 'woman': 4,\n",
       " 'young': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_DIM = len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      king\n",
       "1      king\n",
       "2    strong\n",
       "3    strong\n",
       "4       man\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    strong\n",
       "1       man\n",
       "2      king\n",
       "3       man\n",
       "4      king\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_3:0' shape=(?, 12) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding will be 2 dimension for 2d visualization\n",
    "EMBEDDING_DIM = 2 \n",
    "\n",
    "# hidden layer: which represents word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss function: cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training operation\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "# making placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "\n",
    "# word embedding will be 2 dimension for 2d visualization\n",
    "EMBEDDING_DIM = 2 \n",
    "\n",
    "# hidden layer: which represents word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function: cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
    "\n",
    "# training operation\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss is :  3.3681242\n",
      "iteration 3000 loss is :  1.8271991\n",
      "iteration 6000 loss is :  1.7712481\n",
      "iteration 9000 loss is :  1.7502195\n",
      "iteration 12000 loss is :  1.7378743\n",
      "iteration 15000 loss is :  1.7292285\n",
      "iteration 18000 loss is :  1.7225932\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 20000\n",
    "for i in range(iteration):\n",
    "    # input is X_train which is one hot encoded word\n",
    "    # label is Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 3000 == 0:\n",
    "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07689248 -0.05792753]\n",
      " [ 0.7955911   2.4073446 ]\n",
      " [-0.939559   -0.33133554]\n",
      " [ 0.02298459  3.8780296 ]\n",
      " [ 0.47236913  0.5033327 ]\n",
      " [-1.0396599  -0.19610383]\n",
      " [-5.520339   -0.47706318]\n",
      " [ 1.0928391   5.341041  ]\n",
      " [ 0.25524938  2.5251276 ]\n",
      " [-5.428879   -0.6964346 ]\n",
      " [ 0.10943457  0.8724162 ]\n",
      " [-0.12379886 -3.6496882 ]]\n"
     ]
    }
   ],
   "source": [
    "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>young</td>\n",
       "      <td>-0.076892</td>\n",
       "      <td>-0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>2.407345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boy</td>\n",
       "      <td>-0.939559</td>\n",
       "      <td>-0.331336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>3.878030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.472369</td>\n",
       "      <td>0.503333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>king</td>\n",
       "      <td>-1.039660</td>\n",
       "      <td>-0.196104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>man</td>\n",
       "      <td>-5.520339</td>\n",
       "      <td>-0.477063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wise</td>\n",
       "      <td>1.092839</td>\n",
       "      <td>5.341041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>girl</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>2.525128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prince</td>\n",
       "      <td>-5.428879</td>\n",
       "      <td>-0.696435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.109435</td>\n",
       "      <td>0.872416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>strong</td>\n",
       "      <td>-0.123799</td>\n",
       "      <td>-3.649688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word        x1        x2\n",
       "0      young -0.076892 -0.057928\n",
       "1   princess  0.795591  2.407345\n",
       "2        boy -0.939559 -0.331336\n",
       "3     pretty  0.022985  3.878030\n",
       "4      woman  0.472369  0.503333\n",
       "5       king -1.039660 -0.196104\n",
       "6        man -5.520339 -0.477063\n",
       "7       wise  1.092839  5.341041\n",
       "8       girl  0.255249  2.525128\n",
       "9     prince -5.428879 -0.696435\n",
       "10     queen  0.109435  0.872416\n",
       "11    strong -0.123799 -3.649688"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector in 2d chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2 ))\n",
    "    \n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x_axis_min,x_axis_max)\n",
    "plt.ylim(y_axis_min,y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quakers_nodelist.csv', 'r') as nodecsv: # Open the file                       \n",
    "    nodereader = csv.reader(nodecsv) # Read the csv  \n",
    "    # Retrieve the data (using Python list comprhension and list slicing to remove the header row, see footnote 3)\n",
    "    nodes = [n for n in nodereader][1:]                     \n",
    "\n",
    "node_names = [n[0] for n in nodes] # Get a list of only the node names                                       \n",
    "\n",
    "with open('quakers_edgelist.csv', 'r') as edgecsv: # Open the file\n",
    "    edgereader = csv.reader(edgecsv) # Read the csv     \n",
    "    edges = [tuple(e) for e in edgereader][1:] # Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "print(len(node_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Joseph Wyeth', 'religious writer', 'male', '1663', '1731', '10013191'],\n",
       " ['Alexander Skene of Newtyle',\n",
       "  'local politician and author',\n",
       "  'male',\n",
       "  '1621',\n",
       "  '1694',\n",
       "  '10011149'],\n",
       " ['James Logan',\n",
       "  'colonial official and scholar',\n",
       "  'male',\n",
       "  '1674',\n",
       "  '1751',\n",
       "  '10007567'],\n",
       " ['Dorcas Erbery', 'Quaker preacher', 'female', '1656', '1659', '10003983'],\n",
       " ['Lilias Skene',\n",
       "  'Quaker preacher and poet',\n",
       "  'male',\n",
       "  '1626',\n",
       "  '1697',\n",
       "  '10011152']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joseph Wyeth',\n",
       " 'Alexander Skene of Newtyle',\n",
       " 'James Logan',\n",
       " 'Dorcas Erbery',\n",
       " 'Lilias Skene']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Keith', 'Robert Barclay'),\n",
       " ('George Keith', 'Benjamin Furly'),\n",
       " ('George Keith', 'Anne Conway Viscountess Conway and Killultagh'),\n",
       " ('George Keith', 'Franciscus Mercurius van Helmont'),\n",
       " ('George Keith', 'William Penn')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x121706208>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from(node_names)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 119\n",
      "Number of edges: 174\n",
      "Average degree:   2.9244\n"
     ]
    }
   ],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_sig_dict = {}\n",
    "gender_dict = {}\n",
    "birth_dict = {}\n",
    "death_dict = {}\n",
    "id_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes: # Loop through the list, one row at a time\n",
    "    hist_sig_dict[node[0]] = node[1]\n",
    "    gender_dict[node[0]] = node[2]\n",
    "    birth_dict[node[0]] = node[3]\n",
    "    death_dict[node[0]] = node[4]\n",
    "    id_dict[node[0]] = node[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, hist_sig_dict, 'historical_significance')\n",
    "nx.set_node_attributes(G, gender_dict, 'gender')\n",
    "nx.set_node_attributes(G, birth_dict, 'birth_year')\n",
    "nx.set_node_attributes(G, death_dict, 'death_year')\n",
    "nx.set_node_attributes(G, id_dict, 'sdfb_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joseph Wyeth 1663\n",
      "Alexander Skene of Newtyle 1621\n",
      "James Logan 1674\n",
      "Dorcas Erbery 1656\n",
      "Lilias Skene 1626\n",
      "William Mucklow 1630\n",
      "Thomas Salthouse 1630\n",
      "William Dewsbury 1621\n",
      "John Audland 1630\n",
      "Richard Claridge 1649\n",
      "William Bradford 1663\n",
      "Fettiplace Bellers 1687\n",
      "John Bellers 1654\n",
      "Isabel Yeamans 1637\n",
      "George Fox the younger 1551\n",
      "George Fox 1624\n",
      "John Stubbs 1618\n",
      "Anne Camm 1627\n",
      "John Camm 1605\n",
      "Thomas Camm 1640\n",
      "Katharine Evans 1618\n",
      "Lydia Lancaster 1683\n",
      "Samuel Clarridge 1631\n",
      "Thomas Lower 1633\n",
      "Gervase Benson 1569\n",
      "Stephen Crisp 1628\n",
      "James Claypoole 1634\n",
      "Thomas Holme 1626\n",
      "John Freame 1665\n",
      "John Swinton 1620\n",
      "William Mead 1627\n",
      "Henry Pickworth 1673\n",
      "John Crook 1616\n",
      "Gilbert Latey 1626\n",
      "Ellis Hookes 1635\n",
      "Joseph Besse 1683\n",
      "James Nayler 1618\n",
      "Elizabeth Hooten 1562\n",
      "George Whitehead 1637\n",
      "John Whitehead 1630\n",
      "William Crouch 1628\n",
      "Benjamin Furly 1636\n",
      "Silvanus Bevan 1691\n",
      "Robert Rich 1607\n",
      "John Whiting 1656\n",
      "Christopher Taylor 1614\n",
      "Thomas Lawson 1630\n",
      "Richard Farnworth 1630\n",
      "William Coddington 1601\n",
      "Thomas Taylor 1617\n",
      "Richard Vickris 1590\n",
      "Robert Barclay 1648\n",
      "Jane Sowle 1631\n",
      "Tace Sowle 1666\n",
      "Leonard Fell 1624\n",
      "Margaret Fell 1614\n",
      "George Bishop 1558\n",
      "Elizabeth Leavens 1555\n",
      "Thomas Curtis 1602\n",
      "Alice Curwen 1619\n",
      "Alexander Parker 1628\n",
      "John Wilkinson 1652\n",
      "Thomas Aldam 1616\n",
      "David Barclay of Ury 1610\n",
      "David Barclay 1682\n",
      "Sir Charles Wager 1666\n",
      "George Keith 1638\n",
      "James Parnel 1636\n",
      "Peter Collinson 1694\n",
      "Franciscus Mercurius van Helmont 1614\n",
      "William Caton 1636\n",
      "Francis Howgill 1618\n",
      "Richard Hubberthorne 1628\n",
      "William Ames 1552\n",
      "William Rogers 1601\n",
      "Isaac Norris 1671\n",
      "Anthony Sharp 1643\n",
      "Mary Fisher 1623\n",
      "Anne Conway Viscountess Conway and Killultagh 1631\n",
      "Samuel Fisher 1604\n",
      "Francis Bugg 1640\n",
      "Sarah Gibbons 1634\n",
      "William Tomlinson 1650\n",
      "Humphrey Norton 1655\n",
      "William Gibson 1628\n",
      "Gideon Wanton 1693\n",
      "John Wanton 1672\n",
      "Grace Chamber 1676\n",
      "Mary Prince 1569\n",
      "John Bartram 1699\n",
      "Edward Haistwell 1658\n",
      "John ap John 1625\n",
      "John Rous 1585\n",
      "Anthony Pearson 1627\n",
      "Solomon Eccles 1617\n",
      "John Burnyeat 1631\n",
      "Edward Burrough 1633\n",
      "Rebecca Travers 1609\n",
      "William Edmundson 1627\n",
      "Sarah Cheevers 1608\n",
      "Edward Pyott 1560\n",
      "Daniel Quare 1648\n",
      "John Penington 1655\n",
      "Mary Penington 1623\n",
      "Charles Marshall 1637\n",
      "Humphrey Woolrich 1633\n",
      "William Penn 1644\n",
      "Mary Pennyman 1630\n",
      "Dorothy Waugh 1636\n",
      "David Lloyd 1656\n",
      "Lewis Morris 1671\n",
      "Martha Simmonds 1624\n",
      "John Story 1571\n",
      "Thomas Story 1670\n",
      "Thomas Ellwood 1639\n",
      "William Simpson 1627\n",
      "Samuel Bownas 1677\n",
      "John Perrot 1555\n",
      "Hannah Stranger 1656\n"
     ]
    }
   ],
   "source": [
    "for n in G.nodes(): # Loop through every node, in our data \"n\" will be the name of the person\n",
    "    print(n, G.node[n]['birth_year']) # Access every node by its name, and then by the attribute \"birth_year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network density: 0.02478279447372169\n"
     ]
    }
   ],
   "source": [
    "density = nx.density(G)\n",
    "print(\"Network density:\", density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest path between Fell and Whitehead: ['Margaret Fell', 'George Fox', 'George Whitehead']\n"
     ]
    }
   ],
   "source": [
    "fell_whitehead_path = nx.shortest_path(G, source=\"Margaret Fell\", target=\"George Whitehead\")\n",
    "\n",
    "print(\"Shortest path between Fell and Whitehead:\", fell_whitehead_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of that path: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of that path:\", len(fell_whitehead_path)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Network diameter of largest component: 8\n"
     ]
    }
   ],
   "source": [
    "# If your Graph has more than one component, this will return False:\n",
    "print(nx.is_connected(G))\n",
    "\n",
    "# Next, use nx.connected_components to get the list of components,\n",
    "# then use the max() command to find the largest one:\n",
    "components = nx.connected_components(G)\n",
    "largest_component = max(components, key=len)\n",
    "\n",
    "# Create a \"subgraph\" of just the largest component\n",
    "# Then calculate the diameter of the subgraph, just like you did with density.\n",
    "#\n",
    "\n",
    "subgraph = G.subgraph(largest_component)\n",
    "diameter = nx.diameter(subgraph)\n",
    "print(\"Network diameter of largest component:\", diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triadic closure: 0.16937799043062202\n"
     ]
    }
   ],
   "source": [
    "triadic_closure = nx.transitivity(G)\n",
    "print(\"Triadic closure:\", triadic_closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "nx.set_node_attributes(G, degree_dict, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'historical_significance': 'Quaker leader and founder of Pennsylvania', 'gender': 'male', 'birth_year': '1644', 'death_year': '1718', 'sdfb_id': '10009531', 'degree': 18}\n"
     ]
    }
   ],
   "source": [
    "print(G.node['William Penn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 nodes by degree:\n",
      "('George Fox', 22)\n",
      "('William Penn', 18)\n",
      "('James Nayler', 16)\n",
      "('George Whitehead', 13)\n",
      "('Margaret Fell', 13)\n",
      "('Benjamin Furly', 10)\n",
      "('Edward Burrough', 9)\n",
      "('George Keith', 8)\n",
      "('Thomas Ellwood', 8)\n",
      "('Francis Howgill', 7)\n",
      "('John Perrot', 7)\n",
      "('John Audland', 6)\n",
      "('Richard Farnworth', 6)\n",
      "('Alexander Parker', 6)\n",
      "('John Story', 6)\n",
      "('John Stubbs', 5)\n",
      "('Thomas Curtis', 5)\n",
      "('John Wilkinson', 5)\n",
      "('William Caton', 5)\n",
      "('Anthony Pearson', 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 nodes by degree:\")\n",
    "for d in sorted_degree[:20]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality\n",
    "eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality\n",
    "\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 nodes by betweenness centrality:\n",
      "('William Penn', 0.23999456006192205)\n",
      "('George Fox', 0.23683257726065216)\n",
      "('George Whitehead', 0.12632024847366005)\n",
      "('Margaret Fell', 0.12106792237170329)\n",
      "('James Nayler', 0.10446026280446098)\n",
      "('Benjamin Furly', 0.06419626175167242)\n",
      "('Thomas Ellwood', 0.046190623885104545)\n",
      "('George Keith', 0.045006564009171565)\n",
      "('John Audland', 0.04164936340077581)\n",
      "('Alexander Parker', 0.03893676140525336)\n",
      "('John Story', 0.028990098622866983)\n",
      "('John Burnyeat', 0.028974117533439564)\n",
      "('John Perrot', 0.02829566854990583)\n",
      "('James Logan', 0.026944806605823553)\n",
      "('Richard Claridge', 0.026944806605823553)\n",
      "('Robert Barclay', 0.026944806605823553)\n",
      "('Elizabeth Leavens', 0.026944806605823553)\n",
      "('Thomas Curtis', 0.026729751729751724)\n",
      "('John Stubbs', 0.024316593960227152)\n",
      "('Mary Penington', 0.02420824624214454)\n"
     ]
    }
   ],
   "source": [
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Top 20 nodes by betweenness centrality:\")\n",
    "for b in sorted_betweenness[:20]:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: William Penn | Betweenness Centrality: 0.23999456006192205 | Degree: 18\n",
      "Name: George Fox | Betweenness Centrality: 0.23683257726065216 | Degree: 22\n",
      "Name: George Whitehead | Betweenness Centrality: 0.12632024847366005 | Degree: 13\n",
      "Name: Margaret Fell | Betweenness Centrality: 0.12106792237170329 | Degree: 13\n",
      "Name: James Nayler | Betweenness Centrality: 0.10446026280446098 | Degree: 16\n",
      "Name: Benjamin Furly | Betweenness Centrality: 0.06419626175167242 | Degree: 10\n",
      "Name: Thomas Ellwood | Betweenness Centrality: 0.046190623885104545 | Degree: 8\n",
      "Name: George Keith | Betweenness Centrality: 0.045006564009171565 | Degree: 8\n",
      "Name: John Audland | Betweenness Centrality: 0.04164936340077581 | Degree: 6\n",
      "Name: Alexander Parker | Betweenness Centrality: 0.03893676140525336 | Degree: 6\n",
      "Name: John Story | Betweenness Centrality: 0.028990098622866983 | Degree: 6\n",
      "Name: John Burnyeat | Betweenness Centrality: 0.028974117533439564 | Degree: 4\n",
      "Name: John Perrot | Betweenness Centrality: 0.02829566854990583 | Degree: 7\n",
      "Name: James Logan | Betweenness Centrality: 0.026944806605823553 | Degree: 4\n",
      "Name: Richard Claridge | Betweenness Centrality: 0.026944806605823553 | Degree: 2\n",
      "Name: Robert Barclay | Betweenness Centrality: 0.026944806605823553 | Degree: 3\n",
      "Name: Elizabeth Leavens | Betweenness Centrality: 0.026944806605823553 | Degree: 2\n",
      "Name: Thomas Curtis | Betweenness Centrality: 0.026729751729751724 | Degree: 5\n",
      "Name: John Stubbs | Betweenness Centrality: 0.024316593960227152 | Degree: 5\n",
      "Name: Mary Penington | Betweenness Centrality: 0.02420824624214454 | Degree: 4\n"
     ]
    }
   ],
   "source": [
    "#First get the top 20 nodes by betweenness as a list\n",
    "top_betweenness = sorted_betweenness[:20]\n",
    "\n",
    "#Then find and print their degree\n",
    "for tb in top_betweenness: # Loop through top_betweenness\n",
    "    degree = degree_dict[tb[0]] # Use degree_dict to access a node's degree, see footnote 2\n",
    "    print(\"Name:\", tb[0], \"| Betweenness Centrality:\", tb[1], \"| Degree:\", degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'edges_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-123ca8f3bf65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommunities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36mbest_partition\u001b[0;34m(graph, partition, weight, resolution)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mdendo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dendrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpartition_at_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdendo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdendo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36mgenerate_dendrogram\u001b[0;34m(graph, part_init, weight, resolution)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mstatus_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mcurrent_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minduced_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m     \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/community/community_louvain.py\u001b[0m in \u001b[0;36minduced_graph\u001b[0;34m(partition, graph, weight)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mnode1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcom1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'edges_iter'"
     ]
    }
   ],
   "source": [
    "communities = community.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'communities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8c9940d662c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_node_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modularity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'communities' is not defined"
     ]
    }
   ],
   "source": [
    "nx.set_node_attributes(G, communities, 'modularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'modularity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-640503fae326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First get a list of just the nodes in that class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modularity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Then create a dictionary of the eigenvector centralities of those nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclass0_eigenvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eigenvector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-640503fae326>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First get a list of just the nodes in that class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclass0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'modularity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Then create a dictionary of the eigenvector centralities of those nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclass0_eigenvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eigenvector'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'modularity'"
     ]
    }
   ],
   "source": [
    "# First get a list of just the nodes in that class\n",
    "class0 = [n for n in G.nodes() if G.node[n]['modularity'] == 0]\n",
    "\n",
    "# Then create a dictionary of the eigenvector centralities of those nodes\n",
    "class0_eigenvector = {n:G.node[n]['eigenvector'] for n in class0}\n",
    "\n",
    "# Then sort that dictionary and print the first 5 results\n",
    "class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Modularity Class 0 Sorted by Eigenvector Centrality:\")\n",
    "for node in class0_sorted_by_eigenvector[:5]:\n",
    "    print(\"Name:\", node[0], \"| Eigenvector Centrality:\", node[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
