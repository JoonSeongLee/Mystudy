{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "here I implement word2vec with very simple example using tensorflow  \n",
    "word2vec is vector representation for words with similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data\n",
    "we will use only 10 sentences to create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['king is a strong man', \n",
    "          'queen is a wise woman', \n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong', \n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "In order for efficiency of creating word vector, we will remove commonly used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(\" \".join(tmp))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = remove_stop_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king strong man',\n",
       " 'queen wise woman',\n",
       " 'boy young man',\n",
       " 'girl young woman',\n",
       " 'prince young king',\n",
       " 'princess young queen',\n",
       " 'man strong',\n",
       " 'woman pretty',\n",
       " 'prince boy king',\n",
       " 'princess girl queen']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have word set by which we will have word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy',\n",
       " 'girl',\n",
       " 'king',\n",
       " 'man',\n",
       " 'pretty',\n",
       " 'prince',\n",
       " 'princess',\n",
       " 'queen',\n",
       " 'strong',\n",
       " 'wise',\n",
       " 'woman',\n",
       " 'young'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generation\n",
    "we will generate label for each word using skip gram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy': 2,\n",
       " 'girl': 8,\n",
       " 'king': 5,\n",
       " 'man': 6,\n",
       " 'pretty': 3,\n",
       " 'prince': 9,\n",
       " 'princess': 1,\n",
       " 'queen': 10,\n",
       " 'strong': 11,\n",
       " 'wise': 7,\n",
       " 'woman': 4,\n",
       " 'young': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int = {}\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "\n",
    "\n",
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'strong', 'man'],\n",
       " ['queen', 'wise', 'woman'],\n",
       " ['boy', 'young', 'man'],\n",
       " ['girl', 'young', 'woman'],\n",
       " ['prince', 'young', 'king'],\n",
       " ['princess', 'young', 'queen'],\n",
       " ['man', 'strong'],\n",
       " ['woman', 'pretty'],\n",
       " ['prince', 'boy', 'king'],\n",
       " ['princess', 'girl', 'queen']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in corpus:\n",
    "    sentences.append(sentence.split())\n",
    "    \n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'strong'],\n",
       " ['king', 'man'],\n",
       " ['strong', 'king'],\n",
       " ['strong', 'man'],\n",
       " ['man', 'king'],\n",
       " ['man', 'strong'],\n",
       " ['queen', 'wise'],\n",
       " ['queen', 'woman'],\n",
       " ['wise', 'queen'],\n",
       " ['wise', 'woman'],\n",
       " ['woman', 'queen'],\n",
       " ['woman', 'wise'],\n",
       " ['boy', 'young'],\n",
       " ['boy', 'man'],\n",
       " ['young', 'boy'],\n",
       " ['young', 'man'],\n",
       " ['man', 'boy'],\n",
       " ['man', 'young'],\n",
       " ['girl', 'young'],\n",
       " ['girl', 'woman'],\n",
       " ['young', 'girl'],\n",
       " ['young', 'woman'],\n",
       " ['woman', 'girl'],\n",
       " ['woman', 'young'],\n",
       " ['prince', 'young'],\n",
       " ['prince', 'king'],\n",
       " ['young', 'prince'],\n",
       " ['young', 'king'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'young'],\n",
       " ['princess', 'young'],\n",
       " ['princess', 'queen'],\n",
       " ['young', 'princess'],\n",
       " ['young', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'young'],\n",
       " ['man', 'strong'],\n",
       " ['strong', 'man'],\n",
       " ['woman', 'pretty'],\n",
       " ['pretty', 'woman'],\n",
       " ['prince', 'boy'],\n",
       " ['prince', 'king'],\n",
       " ['boy', 'prince'],\n",
       " ['boy', 'king'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'boy'],\n",
       " ['princess', 'girl'],\n",
       " ['princess', 'queen'],\n",
       " ['girl', 'princess'],\n",
       " ['girl', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'girl']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_SIZE = 2\n",
    "\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for idx, word in enumerate(sentence):\n",
    "        for neighbor in sentence[max(idx - WINDOW_SIZE, 0) : min(idx + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if neighbor != word:\n",
    "                data.append([word, neighbor])\n",
    "                \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king strong man\n",
      "queen wise woman\n",
      "boy young man\n",
      "girl young woman\n",
      "prince young king\n",
      "princess young queen\n",
      "man strong\n",
      "woman pretty\n",
      "prince boy king\n",
      "princess girl queen\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for text in corpus:\n",
    "    print(text)\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['input', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>king</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>king</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strong</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>man</td>\n",
       "      <td>strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>queen</td>\n",
       "      <td>wise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queen</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wise</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wise</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input   label\n",
       "0    king  strong\n",
       "1    king     man\n",
       "2  strong    king\n",
       "3  strong     man\n",
       "4     man    king\n",
       "5     man  strong\n",
       "6   queen    wise\n",
       "7   queen   woman\n",
       "8    wise   queen\n",
       "9    wise   woman"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boy': 2,\n",
       " 'girl': 8,\n",
       " 'king': 5,\n",
       " 'man': 6,\n",
       " 'pretty': 3,\n",
       " 'prince': 9,\n",
       " 'princess': 1,\n",
       " 'queen': 10,\n",
       " 'strong': 11,\n",
       " 'wise': 7,\n",
       " 'woman': 4,\n",
       " 'young': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_DIM = len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      king\n",
       "1      king\n",
       "2    strong\n",
       "3    strong\n",
       "4       man\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    strong\n",
       "1       man\n",
       "2      king\n",
       "3       man\n",
       "4      king\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_3:0' shape=(?, 12) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding will be 2 dimension for 2d visualization\n",
    "EMBEDDING_DIM = 2 \n",
    "\n",
    "# hidden layer: which represents word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss function: cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training operation\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "ONE_HOT_DIM = len(words)\n",
    "\n",
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot_encoding(data_point_index):\n",
    "    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n",
    "    one_hot_encoding[data_point_index] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x, y in zip(df['input'], df['label']):\n",
    "    X.append(to_one_hot_encoding(word2int[ x ]))\n",
    "    Y.append(to_one_hot_encoding(word2int[ y ]))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)\n",
    "\n",
    "# making placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "\n",
    "# word embedding will be 2 dimension for 2d visualization\n",
    "EMBEDDING_DIM = 2 \n",
    "\n",
    "# hidden layer: which represents word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function: cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
    "\n",
    "# training operation\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss is :  3.3681242\n",
      "iteration 3000 loss is :  1.8271991\n",
      "iteration 6000 loss is :  1.7712481\n",
      "iteration 9000 loss is :  1.7502195\n",
      "iteration 12000 loss is :  1.7378743\n",
      "iteration 15000 loss is :  1.7292285\n",
      "iteration 18000 loss is :  1.7225932\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 20000\n",
    "for i in range(iteration):\n",
    "    # input is X_train which is one hot encoded word\n",
    "    # label is Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 3000 == 0:\n",
    "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07689248 -0.05792753]\n",
      " [ 0.7955911   2.4073446 ]\n",
      " [-0.939559   -0.33133554]\n",
      " [ 0.02298459  3.8780296 ]\n",
      " [ 0.47236913  0.5033327 ]\n",
      " [-1.0396599  -0.19610383]\n",
      " [-5.520339   -0.47706318]\n",
      " [ 1.0928391   5.341041  ]\n",
      " [ 0.25524938  2.5251276 ]\n",
      " [-5.428879   -0.6964346 ]\n",
      " [ 0.10943457  0.8724162 ]\n",
      " [-0.12379886 -3.6496882 ]]\n"
     ]
    }
   ],
   "source": [
    "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>young</td>\n",
       "      <td>-0.076892</td>\n",
       "      <td>-0.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>2.407345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boy</td>\n",
       "      <td>-0.939559</td>\n",
       "      <td>-0.331336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty</td>\n",
       "      <td>0.022985</td>\n",
       "      <td>3.878030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>woman</td>\n",
       "      <td>0.472369</td>\n",
       "      <td>0.503333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>king</td>\n",
       "      <td>-1.039660</td>\n",
       "      <td>-0.196104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>man</td>\n",
       "      <td>-5.520339</td>\n",
       "      <td>-0.477063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wise</td>\n",
       "      <td>1.092839</td>\n",
       "      <td>5.341041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>girl</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>2.525128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prince</td>\n",
       "      <td>-5.428879</td>\n",
       "      <td>-0.696435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.109435</td>\n",
       "      <td>0.872416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>strong</td>\n",
       "      <td>-0.123799</td>\n",
       "      <td>-3.649688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word        x1        x2\n",
       "0      young -0.076892 -0.057928\n",
       "1   princess  0.795591  2.407345\n",
       "2        boy -0.939559 -0.331336\n",
       "3     pretty  0.022985  3.878030\n",
       "4      woman  0.472369  0.503333\n",
       "5       king -1.039660 -0.196104\n",
       "6        man -5.520339 -0.477063\n",
       "7       wise  1.092839  5.341041\n",
       "8       girl  0.255249  2.525128\n",
       "9     prince -5.428879 -0.696435\n",
       "10     queen  0.109435  0.872416\n",
       "11    strong -0.123799 -3.649688"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\n",
    "w2v_df['word'] = words\n",
    "w2v_df = w2v_df[['word', 'x1', 'x2']]\n",
    "w2v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector in 2d chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word, x1, x2 in zip(w2v_df['word'], w2v_df['x1'], w2v_df['x2']):\n",
    "    ax.annotate(word, (x1,x2 ))\n",
    "    \n",
    "PADDING = 1.0\n",
    "x_axis_min = np.amin(vectors, axis=0)[0] - PADDING\n",
    "y_axis_min = np.amin(vectors, axis=0)[1] - PADDING\n",
    "x_axis_max = np.amax(vectors, axis=0)[0] + PADDING\n",
    "y_axis_max = np.amax(vectors, axis=0)[1] + PADDING\n",
    " \n",
    "plt.xlim(x_axis_min,x_axis_max)\n",
    "plt.ylim(y_axis_min,y_axis_max)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
